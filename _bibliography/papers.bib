@inproceedings{plaud-etal-2024-revisiting,
    title = "Revisiting Hierarchical Text Classification: Inference and Metrics",
    author = "Plaud, Roman  and
      Labeau, Matthieu  and
      Saillenfest, Antoine  and
      Bonald, Thomas",
    editor = "Barak, Libby  and
      Alikhani, Malihe",
    booktitle = "Proceedings of the 28th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2024",
    address = "Miami, FL, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.conll-1.18/",
    abbr         = {CoNLL},
    doi = "10.18653/v1/2024.conll-1.18",
    pages = "231--242",
    abstract = "Hierarchical text classification (HTC) is the task of assigning labels to a text within a structured space organized as a hierarchy. Recent works treat HTC as a conventional multilabel classification problem, therefore evaluating it as such. We instead propose to evaluate models based on specifically designed hierarchical metrics and we demonstrate the intricacy of metric choice and prediction inference method. We introduce a new challenging dataset and we evaluate fairly, recent sophisticated models, comparing them with a range of simple but strong baselines, including a new theoretically motivated loss. Finally, we show that those baselines are very often competitive with the latest models. This highlights the importance of carefully considering the evaluation methodology when proposing new methods for HTC",
    selected={true},
    preview={example_hwv.png},
    code = {https://github.com/RomanPlaud/revisitingHTC},
    pdf = {https://aclanthology.org/2024.conll-1.18.pdf},
    poster = {poster_conll_.pdf}
}

@misc{vassoyan2025ignoreklpenaltyboosting,
      title={Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning}, 
      author={Jean Vassoyan and NathanaÃ«l Beau and Roman Plaud},
      year={2025},
      eprint={2502.06533},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url= "https://arxiv.org/abs/2502.06533", 
      abbr         = {NAACL}
      abstract = "The ability to achieve long-term goals is a key challenge in the current development of large language models (LLMs). To address this, pre-trained LLMs can be fine-tuned with reinforcement learning (RL) to explore solutions that optimize a given goal. However, exploration with LLMs is difficult, as a balance has to be struck between discovering new solutions and staying close enough to the pre-trained model, so as not to degrade basic capabilities. This is typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we investigate the exploration dynamics of a small language model on a simple arithmetic task. We show how varying degrees of pre-training influence exploration and demonstrate the importance of "critical tokens" which have a dramatic impact on the final outcome. Consequently, we introduce a simple modification to the KL penalty that favors exploration on critical tokens, increasing the efficiency of the RL fine-tuning stage",
      selected={true},
      preview={3-digit-addition-example-6c.gif},
      doi = "10.48550/arXiv.2502.06533"
      code = {https://github.com/jvasso/llm-rl-arithmetic},
      pdf = {https://arxiv.org/pdf/2502.06533}
}
